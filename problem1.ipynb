{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "# Problem 1\n",
    "# n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('Plato_Republic.txt', 'r') as myfile:\n",
    "    myText = myfile.read().replace('\\n',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\machy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿the', 'republic', '.', 'persons', 'of', 'the', 'dialogue', '.']\n",
      "-- Total Number of Words :  136319\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path.append(\"/home/smotsch/Teaching/Deep_net/data_sets/nltk_data\")\n",
    "myText_tokenized = nltk.word_tokenize(myText)\n",
    "myLowerCaseTokens = [word.lower() for word in myText_tokenized]\n",
    "print(myLowerCaseTokens[0:10])\n",
    "print(\"-- Total Number of Words : \",len(myText_tokenized)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Number of Unique words :  7545\n",
      "235\n",
      "<class 'nltk.probability.FreqDist'>\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(myLowerCaseTokens)\n",
    "print(\"-- Number of Unique words : \",len(fdist)) \n",
    "print(fdist['certainly'])\n",
    "print(type(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7545\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for i in fdist.keys():\n",
    "    l.append(i)\n",
    "print(len(l)) #No of unique words - 7545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words:\n",
      "\n",
      "[('certainly', 235), ('knowledge', 152), ('injustice', 111), ('therefore', 109), ('question', 99)]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in fdist.most_common():\n",
    "    if(len(i[0]) >= 8):\n",
    "        result.append(i[0])\n",
    "        if(len(result) >= 5):\n",
    "            break\n",
    "print(\"5 most common words:\\n\")\n",
    "print([(i, fdist[i]) for i in result]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- frequence of the word 'think' :  0\n",
      "-- total number of words :  136319\n",
      "7545\n"
     ]
    }
   ],
   "source": [
    "print(\"-- frequence of the word 'think' : \",fdist[('think',)])    # input must be a one-tuple ('...',) \n",
    "print(\"-- total number of words : \",sum(fdist.values()))\n",
    "print(len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurences of 'I think':  51\n",
      "The 10 most frequent coupled words:\n",
      "\n",
      "[((',', 'and'), 1734), (('of', 'the'), 968), ((',', 'he'), 794), (('.', 'And'), 767), ((',', 'I'), 605), (('to', 'be'), 597), (('he', 'said'), 515), (('in', 'the'), 506), (('said', ','), 506), ((';', 'and'), 452)]\n"
     ]
    }
   ],
   "source": [
    "myBigram = nltk.ngrams(myText_tokenized, 2)\n",
    "fdist_bi = nltk.FreqDist(myBigram)\n",
    "print(\"Occurences of 'I think': \",fdist_bi[('I','think')])\n",
    "print(\"The 10 most frequent coupled words:\\n\")\n",
    "print(fdist_bi.most_common()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(fdist['I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability1(s1, s2): #1.c probability function which takes two words\n",
    "    if fdist[s1.lower()] == 0:\n",
    "        return None\n",
    "    return fdist_bi[(s1,s2)]/fdist[s1.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032217308907138344\n"
     ]
    }
   ],
   "source": [
    "print(probability1('I','think'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(x_seq, x_seq_next):\n",
    "    mat = np.zeros(len(x_seq_next))\n",
    "    for i in range(len(x_seq_next)):\n",
    "        mat[i] = fdist_bi[(x_seq[i],x_seq_next[i])]/fdist[x_seq[i].lower()]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(myText_tokenized):\n",
    "    sum1 = 0\n",
    "    N = len(myText_tokenized)\n",
    "    x_seq = myText_tokenized[0:(N-1)]\n",
    "    x_seq_next = myText_tokenized[1:N]\n",
    "    mat = probability(x_seq, x_seq_next)\n",
    "    mat = np.log(mat)\n",
    "    a = np.sum(mat)\n",
    "    power = (-1/(N-1))*a\n",
    "    print(\"perplexity: \",math.exp(power))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  40.331110196848236\n"
     ]
    }
   ],
   "source": [
    "perplexity(myText_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
