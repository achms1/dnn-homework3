{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3\n",
    "# Problem 2\n",
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Vanilla_RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The vanilla RNN: from (x_t,h_t-1) input,hidden-state\n",
    "        h_t = tanh( R*h_t-1 + A*x_t)\n",
    "        y_t = B*h_t\n",
    "     where A is the encoder, B the decoder, R the recurrent matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Vanilla_RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.A = nn.Linear(input_size, hidden_size)\n",
    "        with torch.no_grad():\n",
    "            self.A.weight.copy_(torch.tensor([[1, -1, -1/2, 1/2],[1, 1, -1/2, -1]],dtype=torch.long))\n",
    "        self.R = nn.Linear(hidden_size, hidden_size)\n",
    "        with torch.no_grad():\n",
    "            self.R.weight.copy_(torch.tensor([[1,0],[0,1]],dtype=torch.long))\n",
    "        self.B = nn.Linear(hidden_size, output_size)\n",
    "        with torch.no_grad():\n",
    "            self.B.weight.copy_(torch.tensor([[1,1], [1/2, 1], [-1,0], [0, -1/2]],dtype=torch.long))\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        # update the hidden state\n",
    "        h_update = self.tanh( self.R(h) + self.A(x) )\n",
    "        # prediction\n",
    "        y = self.B(h_update)\n",
    "        return y,h_update\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LETTERS ='helo'\n",
    "NB_LETTERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def letterToIndex(letter):\n",
    "    \"\"\" Find letter index from all_letters, e.g. \"a\" = 0 \"\"\"\n",
    "    #print(\"found: \", ALL_LETTERS.find(letter))\n",
    "    return ALL_LETTERS.find(letter)\n",
    "\n",
    "def letterToTensor(letter):\n",
    "    \"\"\" Transform a letter into a 'hot-vector' (tensor) \"\"\"\n",
    "    #tensor = torch.zeros(1, NB_LETTERS,dtype=torch.long)\n",
    "    tensor = torch.zeros(1, NB_LETTERS)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "#print(\"Embedding of the character 'c':\")\n",
    "#letterToTensor('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = {'n_steps' : 1000,\n",
    "     'lr' : .005,\n",
    "     'chunk_len' : 5}\n",
    "myText='hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictChars(myRnn, myText, P):\n",
    "    h = myRnn.init_hidden()\n",
    "    start_index=0\n",
    "    end_index=len(myText)-1\n",
    "    result=\"\"\n",
    "    for p in range(P['chunk_len']):\n",
    "        x = letterToTensor( myText[p] )\n",
    "        y, h = myRnn(x, h)\n",
    "        print(y)\n",
    "        print(\"input: \",myText[p])\n",
    "        predicted_char = ALL_LETTERS[y.argmax()]\n",
    "        print(\"output: \",predicted_char)\n",
    "        result += predicted_char\n",
    "    print(str.format(\"final prediction for 5 chars is {}\",result))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4481,  1.0917, -0.6069, -0.6729]], grad_fn=<AddmmBackward>)\n",
      "input:  h\n",
      "output:  h\n",
      "tensor([[ 0.8374,  1.1223,  0.0344, -0.6729]], grad_fn=<AddmmBackward>)\n",
      "input:  e\n",
      "output:  e\n",
      "tensor([[ 1.1069,  1.0913, -0.2662, -0.6729]], grad_fn=<AddmmBackward>)\n",
      "input:  l\n",
      "output:  h\n",
      "tensor([[ 1.2844,  1.0890, -0.4459, -0.6729]], grad_fn=<AddmmBackward>)\n",
      "input:  l\n",
      "output:  h\n",
      "tensor([[ 1.1451,  0.8730, -0.5227, -0.6729]], grad_fn=<AddmmBackward>)\n",
      "input:  o\n",
      "output:  h\n",
      "final prediction for 5 chars is hehhh\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 2\n",
    "in_out_size = NB_LETTERS\n",
    "myRnn = Vanilla_RNN(in_out_size,hidden_size,in_out_size)\n",
    "predictChars(myRnn,myText,P)\n",
    "match = \"olleh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "def train_RNN(myRnn,myText,P,shouldPrintTraining=False):\n",
    "    \"\"\"\n",
    "    Train a recurrent neural network from a text ('myText'). The dictionary P\n",
    "    should contain:\n",
    "       . the learning rate 'lr'\n",
    "       . the number of steps 'n_steps'\n",
    "       . the size of the sentence trained on 'chunk_len'\n",
    "    \"\"\"\n",
    "    # init\n",
    "    optimizer = torch.optim.Adam(myRnn.parameters(), lr=P['lr'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    df = pd.DataFrame(columns=('step', 'loss'))\n",
    "    # train\n",
    "    for step in range(P['n_steps']):\n",
    "        # A) initialize\n",
    "        h = myRnn.init_hidden()\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        # B) pick a chunk from the text\n",
    "        start_index = random.randint(0, len(myText) - P['chunk_len'])\n",
    "        end_index = start_index + P['chunk_len'] + 1\n",
    "        chunk = myText[start_index:end_index]\n",
    "        if (shouldPrintTraining) & (step%50 == 0):\n",
    "            print(\" input  = \", chunk)\n",
    "            chunk_predicted=\"\"\n",
    "            #chunk_predicted = chunk[0]\n",
    "        # C) prediction\n",
    "        for p in range(P['chunk_len']):\n",
    "            # init\n",
    "            x = letterToTensor( chunk[p] )\n",
    "            #x_next = letterToTensor( chunk[p+1] )\n",
    "            letter_x_next = letterToIndex(match[p])\n",
    "            #print(chunk[p+1])\n",
    "            #print(letter_x_next)\n",
    "            target = torch.tensor([letter_x_next],dtype=torch.long)\n",
    "            # prediction\n",
    "            y, h = myRnn(x, h)\n",
    "            # loss\n",
    "            loss += criterion(y.view(1,-1), target)\n",
    "            if (shouldPrintTraining):\n",
    "                chunk_predicted += ALL_LETTERS[y.argmax()]\n",
    "        # D) gradient step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # E) save loss\n",
    "        ave_loss = loss.detach().numpy() / P['chunk_len']\n",
    "        if (shouldPrintTraining) & (step%50 == 0):\n",
    "            print(\" output = \", chunk_predicted)\n",
    "        df.loc[step] = [step, ave_loss]\n",
    "        if (step%50 == 0):\n",
    "            # print only once every 50 steps\n",
    "            print('loss at step ',str(step),' : ', str(ave_loss))\n",
    "    # result\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input  =  hello\n",
      " output =  heeel\n",
      "loss at step  0  :  1.6518274307250977\n",
      " input  =  hello\n",
      " output =  hllll\n",
      "loss at step  50  :  1.2891677856445312\n",
      " input  =  hello\n",
      " output =  hlllh\n",
      "loss at step  100  :  1.0270138740539552\n",
      " input  =  hello\n",
      " output =  hlllh\n",
      "loss at step  150  :  0.8214122772216796\n",
      " input  =  hello\n",
      " output =  olllh\n",
      "loss at step  200  :  0.5997264862060547\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  250  :  0.3920018196105957\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  300  :  0.26860618591308594\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  350  :  0.20167596340179444\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  400  :  0.15901960134506227\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  450  :  0.12925336360931397\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  500  :  0.10735998153686524\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  550  :  0.09068173170089722\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  600  :  0.0776459038257599\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  650  :  0.06725101470947266\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  700  :  0.05882502794265747\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  750  :  0.05189940929412842\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  800  :  0.046137934923171996\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  850  :  0.04129338264465332\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  900  :  0.03718062043190003\n",
      " input  =  hello\n",
      " output =  olleh\n",
      "loss at step  950  :  0.033658814430236814\n"
     ]
    }
   ],
   "source": [
    "myRnn = Vanilla_RNN(in_out_size,hidden_size,in_out_size)\n",
    "df = train_RNN(myRnn,myText,P,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.4028, -2.0216, -1.0512,  1.1715],\n",
      "        [-1.1526,  0.1177,  1.6872, -0.1156]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5203,  2.5191],\n",
      "        [-2.3419,  3.3373],\n",
      "        [-3.0594, -1.7435],\n",
      "        [ 1.9378, -2.0168]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2546,  1.5832],\n",
      "        [-1.3031,  2.7908]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(myRnn.A.weight)\n",
    "print(myRnn.B.weight)\n",
    "print(myRnn.R.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a RNN\n",
    "\n",
    "#     hidden_size = 2\n",
    "#     in_out_size = NB_LETTERS\n",
    "#     A = torch.tensor([[1, -1, -1/2, 1/2],[1, 1, -1/2, -1]])\n",
    "#     R = torch.tensor([[1,0],[0,1]])\n",
    "#     B = torch.tensor([[1,1], [1/2, 1], [-1,0], [0, -1/2]])\n",
    "#     myRnn = Vanilla_RNN(in_out_size,hidden_size,in_out_size)\n",
    "\n",
    "# test it\n",
    "#     x_0 = letterToTensor('l')\n",
    "#     h = myRnn.init_hidden()\n",
    "#     y_0,h = myRnn(x_0,h)\n",
    "#     print(y_0)#pick the one letter which has the highest score associated\n",
    "#     print(\"input: \",'T')\n",
    "#     predicted_char = ALL_LETTERS[y_0.argmax()]\n",
    "#     print(\"output: \",predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
